# 토요일 오후 1시 49분, SCG의 모든 서비스가 멈췄다

> **2026년 2월 21일 토요일.** 성균관대학교 개발 학술단체 SCG는 그날 오후, 예상치 못한 전면 장애를 겪었다. 서버 코드 한 줄 잘못된 것도 아니었고, 배포 실수도 아니었다. 진짜 원인은... 엘리베이터 옆에 붙어 있던 종이 한 장이었다.

**🕐 장애 시간:** 13:49 ~ 15:02 (약 1시간 13분)  
**💥 영향 범위:** scg.skku.ac.kr 전체 서비스 (홈페이지, 어플라이 등)  
**🔍 원인:** 제2공학관 네트워크 장비 교체 공사로 인한 물리적 네트워크 단절

---

## "혹시 scg.skku.ac.kr 접속 다들 잘 되시나요?"

오후 1시 49분. SCG Slack의 `#proj-인프라` 채널에 메시지 하나가 올라왔다.

겉보기엔 평범한 질문이었다. 하지만 이 한 줄짜리 메시지가 올라오는 순간, 인프라팀의 토요일 오후는 완전히 다른 방향으로 흘러가기 시작했다.

"저도 안 되네요."

1분도 채 안 돼 답장이 달렸다. 그리고 팀원들이 하나둘 확인하기 시작했다.

> *DNS 문제인가? 서버가 다운됐나? 설마… 물리적으로?*

SSH 접속도 안 된다는 보고가 들어왔다. PM1~5 서버가 전부 응답 없음. 브라우저에 뜬 화면은 냉정했다.

```
ERR_NAME_NOT_RESOLVED
scg.skku.ac.kr's server IP address could not be found.
```

평소라면 DNS 이슈, BIND9 설정 문제, 네트워크 라우팅 오류 등 수십 가지 가능성을 놓고 디버깅을 시작했을 것이다. 하지만 이번엔 달랐다. 범인은 코드 어딘가에 있지 않았다.

---

## 5분 만의 트리아지: 범위를 빠르게 좁혀라

팀원들이 각자 맡은 영역을 빠르게 확인하기 시작했다.

- **이현우:** PM1~5 전부 응답 없음 확인 중
- **양현준:** "115.145.150.x 접속되면 — K8s는 됩니다"
- **장재원:** "2공 서버실만 내려간 것 같은데요"

이 짧은 핑퐁 속에서 상황이 정리됐다. K8s 클러스터는 살아있었다. DNS 서버가 올라가 있는 **제2공학관 서버실**만 죽어있었다. 장애 범위가 순식간에 좁혀졌다. 5분도 안 걸렸다.

그리고 장재원은 학교에서 본 안내문 하나를 떠올렸다.

> *"이거 학교에서 무슨 통신선 공사 어쩌구... 안내문을 본 것 같아요"*

---

## 범인의 정체: 엘리베이터 옆 종이 한 장

양현준이 직접 서버실로 달려갔다. 그리고 엘리베이터 옆 벽에서 발견한 것은—

**전산망(인터넷) 서비스 일시중지 안내**

> 정보통신팀에서는 전산망 성능 향상을 위하여 노후 네트워크 장비 교체 작업을 진행합니다.
>
> - **대상건물:** 제2공학관
> - **작업일시:** 2026. 2. 21.(토) 09:00 ~ 24:00
> - **작업내용:** 노후 네트워크장비 교체

학교 정보통신처가 토요일 하루 종일 제2공학관 네트워크 장비를 교체하고 있었다. SCG의 주요 서버들이 올라가 있는 바로 그 건물에서.

Slack에서 장재원의 반응은 간결했다.

> "아"  
> ".. 저거다"  
> "하...."  
> "ㅋㅋ.."

이 네 줄에 모든 것이 담겨 있었다. 황당함, 안도(적어도 서버가 고장난 건 아니니까), 그리고 약간의 자조. 실제로 서버실에 들어가 보니 서버들은 멀쩡하게 켜져 있었다. 복도엔 교체될 낡은 Cisco 스위치 장비들이 쌓여 있었다.

서버 문제가 아니었다. 서버와 외부 세계를 연결하는 **네트워크 선 자체**가 공사로 인해 끊긴 것이었다.

---

## 아무것도 할 수 없는 상황에서의 의사결정

장재원은 운전 중에도 Slack으로 계속 상황을 체크했다. 팀원들이 논의한 선택지는 크게 세 가지였다.

**옵션 1: 그냥 기다린다**  
공사는 자정 안에 끝날 예정. 서버는 살아있고, 네트워크만 복구되면 모든 서비스가 자동으로 올라온다. 괜히 손댔다가 공사 끝난 후 새로운 문제가 생길 수 있다.

**옵션 2: DNS를 1공학관 서버로 임시 이전한다**  
K8s 클러스터는 살아있으니 DNS 서버 IP만 1공으로 바꾸면 서비스 복구 가능. 하지만 `scg.skku.ac.kr` 도메인의 NS 레코드를 바꾸려면 학교 정보통신처에 연락해야 하는데... 오늘은 토요일 휴일이다.

**옵션 3: Cloudflare의 scg.sh 도메인으로 임시 우회한다**  
에브리타임 홍보 글에 `scg.sh` 도메인도 넣어뒀으니 이걸 활용하는 방법. 하지만 어플라이의 백엔드 도메인과 SQL 주소가 기존 도메인으로 하드코딩돼있어서, hosts 파일 수정이나 재배포가 필요했다.

팀 내에서 진지하게 옵션 2, 3을 검토했지만, 결론은 **옵션 1**로 수렴했다. 이유는 명확했다.

> *"혹시나 공사는 끝났는데 저희가 가한 수정 때문에 다시 또 뭐가 안 되면..."*

프로덕션 시스템에서 가장 위험한 건 **불필요한 변경**이다. 안정적인 복구가 예정돼있고, 리스크 있는 수동 조치의 이득이 크지 않다면? 기다리는 게 맞다. 이건 단순히 소극적인 태도가 아니라, 엔지니어링적으로 올바른 판단이었다.

---

## 오후 3시 02분: 갑작스러운 복구

팀이 각자 대기하던 오후 3시 02분.

**한태혁:** "오잉"  
**한태혁:** "http://scg.skku.ac.kr 되네요"

**장재원:** "ㅇㅅㅇ?"  
**장재원:** "공사 끝났나봐요"  
**장재원:** ";;;;;;;;;;;ㅋ"

공사 예정 종료 시간은 자정이었는데, 오후 3시에 이미 복구됐다. 어플라이도 정상 확인. 모든 서비스 정상 동작. 총 장애 시간은 약 **1시간 13분**.

그리고 Slack의 health-check 봇이 복구 알림을 아주 충실하게 보내왔다.

---

## 장애가 끝난 후: 진짜 중요한 이야기

서비스가 복구되고 나서, 장재원은 바로 회고를 시작했다. 여기서 SCG가 다른 것 같다고 느꼈다.

단순히 "다행히 해결됐네~"로 끝내지 않았다.

> *"health-check가 더 일찍 체크하지 못한 이유는 회고해볼 만하네요"*

두 가지 교훈이 정리됐다.

### 🔎 교훈 1: 공사 사실이 전달되지 않았다

학교 측이 안내문을 붙였지만, 서버실을 사용하는 단체들에게는 별도 통보가 없었다. 이건 학교 측의 프로세스 문제이기도 하지만, SCG 측에서도 학교 인프라 공지를 모니터링하는 체계가 없었다는 게 드러났다.

장재원은 이 부분에서 솔직하게 인정했다.

> *"이건 제가 생각을 못한 것도 있습니다. 단순히 아 웍실에서 작업 못하겠구나 생각에 그친 저의 불찰 죄송합니다."*

팀 리더가 공개 채널에서 자신의 실수를 이렇게 솔직하게 인정한다. 쉬운 일이 아니다.

### 🔎 교훈 2: 모니터링의 단일 장애점(SPOF) 문제

Slack의 `#health-check` 채널이 있었지만, 정작 health-check 서비스 자체도 제2공학관 서버 위에서 돌고 있었다. 모니터링하는 서버와 모니터링 당하는 서버가 같은 곳에 있었으니, 서버실이 통째로 끊기자 모니터링도 같이 죽어버린 것.

이건 단순한 실수가 아니다. **모니터링 인프라의 단일 장애점** 문제다. 프로덕션 환경에서 반드시 분리해야 하는 설계 원칙인데, 학생 단체 환경에서 이 트레이드오프를 직접 겪고 배운 것이다.

복구 알림은 아주 잘 왔다고 했다. 그런데 아이러니하게도, 가장 중요한 순간인 **장애 발생 시점**에는 알림이 오지 않았다. 모니터링이 죽어있었으니까.

---

## SCG 인프라, 사실 이렇게 돌아간다

이번 장애를 통해 SCG의 인프라 구조가 어느 정도 드러났다. 학교 안에서 꽤 실제적인 수준의 시스템을 운영하고 있다.

**DNS 서버 (BIND9):** 단순히 AWS Route53 쓰는 게 아니라, 직접 네임서버를 올리고 운영한다. `bindizr`라는 자체 DNS 관리 도구까지 붙어있다. 전날 새벽 12시에도 DNS 관련 작업 커밋이 올라갔을 만큼 활발하게 개선 중.

**Kubernetes 클러스터:** 별도 서버에서 운영 중인 K8s가 이번 장애에서 멀쩡히 살아남았다. 서비스들이 컨테이너로 올라가 있어, DNS만 복구되면 바로 접근 가능한 구조 덕분에 복구가 깔끔했다.

**온프레미스 물리 서버:** PM1~PM5로 명명된 서버들이 제2공학관 서버실에 설치돼있다. 데이터베이스도 이 서버들 중 일부에서 운영. 클라우드만 쓰는 게 아니라 직접 서버를 관리한다.

**Cloudflare (scg.sh):** 보조 도메인이 Cloudflare를 통해 관리돼있어, 비상시 우회 경로로 활용 가능한 이중화 구조가 어느 정도 갖춰져 있었다.

대기업 못지않은 레이어드 아키텍처다. 물론 완벽하지 않고, 이번처럼 예상 못한 장애도 나지만 — 이런 규모의 시스템을 학생이 직접 설계하고 운영한다는 게 핵심이다.

---

## 그날 서버실엔 우리만 있지 않았다

장재원이 도착한 서버실에는, SCG 팀원들만 있는 게 아니었다.

> *"다 저희같이 부랴부랴 오신 분들 서버실 안에 보이네요;;; 어우"*

비슷한 상황의 다른 학내 단체 운영진들도 각자 서버 확인하러 달려온 것이다. 학교 서버실을 쓰는 단체들이 모두 같은 처지. 토요일 오후에 학교 서버실에 모인 개발자들. 묘하게 연대감이 느껴지는 장면이었을 것 같다.

서버엔 SCG의 관리자 정보 스티커가 붙어있다. 이번 기회에 업데이트도 됐다. 이런 것 하나하나에서 "우리 서버를 진짜로 우리가 관리한다"는 게 느껴진다.

---

## 마치며: 장애는 가장 솔직한 교과서다

카카오, 토스, 라인 같은 곳의 포스트모텀 문화를 들어본 적 있을 거다. 장애가 나면 원인을 분석하고, 재발 방지책을 만들고, 이를 조직 전체에 공유한다. blame 없이 시스템을 개선하는 문화.

SCG는 그걸 학교 안에서 하고 있다.

오늘의 장애는 사실 굉장히 단순한 원인이었다. 누군가의 코드 실수도, 설계 결함도 아닌, 그냥 학교 공사. 하지만 이 한 번의 경험에서 팀은 실제로 배웠다.

- DNS 서버와 health-check 서버의 물리적 분리
- 학교 인프라 공지를 모니터링하는 체계의 필요성
- 모니터링 인프라 자체의 SPOF 제거
- 장애 중 의사결정 원칙 (최소 변경, 복구 예측 시 관망)
- 서버실 관리자 연락망 최신화

이런 경험은 책에서 읽는 것과 다르다. 토요일 오후에 Slack 알림 보고 실제로 서버실로 달려가면서 느끼는 것과는 비교가 안 된다.

SCG에서는 이런 일이 가끔 일어난다. 그리고 그때마다 우리는 조금 더 나은 엔지니어가 된다.

---

*이 글은 2026년 2월 21일 발생한 SCG 서비스 장애에 대한 회고 포스트입니다.*  
*총 장애 시간: 약 1시간 13분. 관련 인프라 개선 작업은 진행 중입니다.*

*궁금한 점이 있거나, 같이 이런 걸 만들어보고 싶다면 — SCG의 문은 열려 있습니다. 🚪*
