# 🔥 월요일 오후 1시 42분, 서버가 조용히 죽었다

**— 디스크 100%가 만든 30분간의 장애 대응기**

> _"디스크가 저 상태로 버티는 게 신기한 거였네요"_
> — 장애 원인을 발견한 직후 슬랙에 올라온 메시지

---

## 🚨 프롤로그: 봇이 먼저 알았다

2026년 2월 23일 월요일, 점심을 먹고 나른해질 시간.

SCG의 `#health-check` 채널에 노란색 경고가 울렸습니다.

```
⚠️ [WARNING] 2026-02-23 13:42:41 (KST)
https://eeegrd.skku.edu 로의 요청이 5회 연속 실패했습니다.
```

성균관대 전자전기공학부 학부 졸업전시 사이트, `eeegrd.skku.edu`.
외부에서 접속하면 **Nginx 502 Bad Gateway**만 덩그러니 떠 있었습니다.

"에이, 잠깐 그런 거겠지" 하고 넘기기엔, 봇은 냉정했습니다.
9분 뒤 상황은 FATAL로 격상됩니다.

```
🚨 [FATAL] 2026-02-23 13:51:12 (KST)
https://eeegrd.skku.edu 로의 요청이 10분 이상 실패했습니다.
```

10분 넘게 죽어 있다. 이건 "잠깐 그런 거"가 아니었습니다.

---

## 🕐 13:53 — 긴급 호출

장재원(인프라팀)이 슬랙에 시스템 장애를 선언합니다.

> 🚨 **시스템 장애가 발생했습니다.** @인프라팀 @운영진

노션에서 해당 서비스 정보를 꺼내 봅니다. 배포 방식은 **Portainer(Docker 웹 UI)**를 통한 컨테이너 배포라고 적혀 있었는데…

**Portainer에 해당 컨테이너가 없습니다.**

<!-- 📸 [Image 1: Portainer 컨테이너 목록 스크린샷 — scg-2026-frontend, scg-new-2022-be 등은 보이지만 eeegrd 관련 컨테이너는 없음] -->

running 상태인 컨테이너 3개. `scg-2026-frontend`, `scg-new-2022-be`, `portainer` 자기 자신.
**졸업전시 사이트의 흔적은 어디에도 없었습니다.**

---

## 🔍 14:00 — 유령 서비스를 찾아서

노션 문서에는 Portainer로 배포했다고 적혀 있지만, Portainer에는 없다.
그렇다면 방법은 하나. **직접 서버에 붙어서 찾는 수밖에.**

SSH로 서버에 접속한 장재원은 먼저 PM2 목록을 확인합니다.

<!-- 📸 [Image 2 또는 3: PM2 list 스크린샷 — icc-haedong-reservation(stopped), ice-gp-manage(online), s2022front(stopped), scg-apply(stopped), seminar(online)] -->

PM2에서 관리 중인 프로세스 5개. 하지만 여기에도 `eeegrd` 관련 서비스는 없습니다.

**Portainer에도 없고, PM2에도 없다.**

그러면 도대체 이 서비스는 어디서 돌고 있었던 걸까요?

Nginx 설정을 열어봅니다.

```nginx
server {
    server_name eeegrd.skku.edu;
    listen 443 ssl;

    location / {
        proxy_pass http://localhost:8080;
    }
}
```

Nginx는 `eeegrd.skku.edu`로 들어오는 요청을 **localhost:8080**으로 프록시하고 있었습니다.
그런데 8080 포트에서는… **아무것도 안 돌고 있었습니다.**

문이 닫힌 가게에 손님을 보내고 있었던 겁니다. 502 Bad Gateway가 뜰 수밖에요.

---

## 🕵️ 미스터리: 그러면 원래 뭐가 8080에서 돌았나?

프로젝트 디렉토리를 뒤져봅니다.

```bash
$ cd ~/Projects/EEE-undergrad-graduation-exhibition
$ grep -r "808" ./docker-compose.prod.yaml
```

```yaml
ports:
  - '8080:3000'
```

**Docker Compose**로 배포된 서비스였습니다. 호스트의 8080 포트를 컨테이너 내부의 3000 포트에 매핑하는 구조.

즉, 이 서비스는 Portainer를 통해서가 아니라 **docker-compose CLI**로 직접 배포된 거였고, 그래서 Portainer UI에 나타나지 않았던 겁니다.

그리고 이 컨테이너가 어느 시점에 **조용히 죽어버린 것**이죠.

---

## ⚡ 14:10 — 소생 시도

원인 분석은 나중에 하고, 일단 살리는 게 먼저입니다.

```bash
$ sudo docker-compose -f docker-compose.prod.yaml up -d
```

이미지 빌드가 시작됩니다. `node:16.16.0` 베이스 이미지 위에 `npm install`이 돌아가는 동안의 약 3분.

서버실 어딘가에서 팬이 돌아가는 소리만 들리는 긴장의 시간이었을 겁니다.

```
[+] Running 1/1
 ✔ Container app  Started
```

**14:13, 소생 완료.**

```
✅ [RECOVERY] 2026-02-23 14:13:43 (KST)
https://eeegrd.skku.edu 로의 요청이 복구 되었습니다.
```

장애 인지부터 복구까지 약 **30분**. health-check 봇이 복구를 확인하고 초록색 체크를 찍어줍니다.

**하지만 이야기는 여기서 끝나지 않았습니다.**

---

## 💀 14:23 — 진짜 범인을 찾다

> "근데 갑자기 이게 죽을 수가 있나"

양현준이 과거에 PM2 명령어를 치다가 서비스를 몇 개 내린 적이 있다고 했지만, 이번엔 아무도 아무것도 안 건드렸습니다.

서비스가 "지 혼자" 죽은 겁니다.

장재원이 디스크 상태를 확인하는 명령어를 칩니다.

```bash
$ df -h
```

```
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda1       156G  147G  917M 100% /
```

**100%.**

156GB짜리 루트 파티션이 **꽉 차 있었습니다.** 남은 공간 917MB. 사실상 0에 가까운 상태.

> "디스크가 저 상태로 버티는 게 신기한 거였네요"

이 한 줄에 모든 것이 담겨 있었습니다.

---

## 🧩 퍼즐 맞추기: 왜 디스크가 꽉 찼는가

디스크가 꽉 차면 무슨 일이 벌어질까요?

- MySQL이 트랜잭션 로그를 쓰지 못해 **커넥션을 거부**합니다
- Docker 컨테이너가 내부적으로 파일을 쓰려다 **I/O 에러**로 뻗습니다
- Nginx는 멀쩡하지만, 뒤에서 요청을 받아줄 프로세스가 없으니 **502를 뱉습니다**
- 심지어 SSH 접속조차 불안정해질 수 있습니다

그렇다면, **156GB를 누가 다 먹었는가?**

범인을 추적합니다.

```bash
$ du -sh ~/.pm2/logs
21G    /home/scg/.pm2/logs
```

**PM2 로그, 21GB.**

PM2로 관리 중인 프로세스들이 뿜어낸 로그가 아무런 제한 없이 쌓이고 또 쌓여, 21기가바이트라는 거대한 덩어리가 되어 있었습니다.

여기에 Docker 컨테이너 로그, Docker 이미지 레이어, 그 외 시스템 로그들까지 합치면 156GB는 진작에 꽉 찼어야 합니다.

이건 어제오늘의 문제가 아니었습니다. **수개월, 어쩌면 수년간** 조용히 쌓여온 시한폭탄이었죠.

---

## 🧹 14:32 — 응급 수술

```bash
$ pm2 flush
```

```bash
$ du -sh ~/.pm2/logs
4.0K    /home/scg/.pm2/logs
```

**21GB → 4KB.** 한 줄의 명령어로 숨통이 트입니다.

이어서 Docker 로그 제한 설정에 대한 논의가 시작됩니다.

```json
// /etc/docker/daemon.json
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "20m",
    "max-file": "3"
  }
}
```

> 위 설정을 적용하면, 각 컨테이너의 로그가 20MB × 3개 = **최대 60MB**를 넘지 않게 물리적으로 차단됩니다.

그리고 PM2 쪽에도 자동 로그 로테이션 모듈을 설치합니다.

```bash
$ pm2 install pm2-logrotate
$ pm2 set pm2-logrotate:max_size 10M
$ pm2 set pm2-logrotate:retain 7
$ pm2 set pm2-logrotate:compress true
$ pm2 set pm2-logrotate:rotateInterval '0 0 * * *'
```

10MB가 넘으면 파일을 쪼개고, 최대 7개까지만 보관하고, 오래된 건 압축하는 설정.

**일단 급한 불은 껐습니다.**

---

## 📊 사건 타임라인 정리

| 시각  | 이벤트                                          |
| ----- | ----------------------------------------------- |
| 13:42 | ⚠️ health-check 봇 WARNING — 5회 연속 요청 실패 |
| 13:51 | 🚨 health-check 봇 FATAL — 10분 이상 장애 지속  |
| 13:53 | 장재원, 슬랙에 시스템 장애 선언                 |
| 13:56 | Portainer에서 해당 컨테이너를 찾을 수 없음 확인 |
| 14:00 | SSH로 서버 직접 접속, 원인 추적 시작            |
| 14:09 | **8080 포트에서 아무것도 안 돌고 있음** 확인    |
| 14:10 | docker-compose로 서비스 재빌드 시작             |
| 14:13 | ✅ 서비스 복구 완료, health-check 봇 RECOVERY   |
| 14:23 | **디스크 사용량 100%** 발견 — 근본 원인 특정    |
| 14:32 | PM2 로그 **21GB** 확인 후 flush                 |
| 14:45 | pm2-logrotate 설정 완료                         |

**장애 지속 시간: 약 31분**
**근본 원인: 디스크 풀(Disk Full)로 인한 Docker 컨테이너 비정상 종료**

---

## 🤔 이 사건이 우리에게 던지는 질문들

서비스는 복구됐고, 당장의 재발 방지책도 적용했습니다.
하지만 이번 장애는 단순한 "로그 좀 지워야지"를 넘어서, SCG 인프라 운영 전반에 대한 질문을 던집니다.

### 1. "이 서비스 어디서 돌아가고 있었죠?"

이번 장애 대응에서 가장 시간을 잡아먹은 건, 서비스를 살리는 게 아니라 **서비스를 찾는 것**이었습니다.

- Portainer에도 없고
- PM2에도 없고
- 노션 문서와 실제 배포 상태가 다르고

SCG 서버에는 다양한 세대의 프로젝트들이 공존합니다. 누군가는 Docker Compose로, 누군가는 PM2로, 또 누군가는 `nohup node app.js &`로 프로세스를 띄워놓고 졸업했을 수도 있습니다.

**포트 점유 현황을 조사해보니:**

| 포트 | 프로세스            | 관리 도구          | 실행 계정 | 비고                      |
| ---- | ------------------- | ------------------ | --------- | ------------------------- |
| 8080 | eeegrd (죽어있었음) | Docker Compose     | scg       | 졸업전시                  |
| 8089 | iccys               | 없음 (생 프로세스) | scg       | Jan 19부터 실행           |
| 8090 | server/app.js       | 없음 (생 프로세스) | **root**  | 2025년부터 실행, Node v10 |
| 8091 | seminar             | 없음 (생 프로세스) | scg       | 2025년부터 실행           |
| 9002 | scg-new-2022-be     | Docker (Portainer) | root      | —                         |
| 8000 | scg-2026-frontend   | Docker (Portainer) | root      | —                         |

특히 8090 포트의 프로세스는 **root 권한**으로 **Node.js v10**(2021년에 EOL된 버전)을 돌리고 있었습니다. 보안적으로도, 운영적으로도 상당히 위험한 상태입니다.

> 💡 **우리가 풀어야 할 숙제:** 모든 서비스의 배포 방식, 포트, 관리 도구, 담당자를 한눈에 볼 수 있는 **Service Registry**가 필요합니다. 노션 문서가 최신 상태를 반영하지 못한다면, 차라리 서버에서 자동으로 수집하는 방식을 고민해봐야 합니다.

### 2. "로그는 자산인가, 부채인가?"

이번 장애의 직접적 원인은 **21GB짜리 PM2 로그**였습니다.

pm2-logrotate를 설치한 건 좋은 첫걸음이지만, 이것도 완벽한 해법은 아닙니다.

**pm2-logrotate의 한계:**

- 그 자체가 Node.js 프로세스 하나를 더 띄우는 것이라 **리소스 오버헤드**가 있습니다
- PM2 데몬이 죽으면 logrotate도 같이 죽어서, 로그가 다시 무한 증식할 수 있습니다
- 자정에 한꺼번에 gzip 압축이 돌면 **I/O 스파이크**가 발생할 수 있습니다
- 로그가 7개 파일로 쪼개져 있으면 장애 분석 시 **여러 파일을 합쳐서 분석해야 하는** 번거로움이 생깁니다

현대적인 인프라에서 로그는 **로컬 디스크에 쌓는 것이 아니라, 스트림으로 흘려보내는 것**이 정석입니다.

> 💡 **우리가 고민해봐야 할 것:** 중앙화된 로그 수집 시스템(Grafana Loki, ELK Stack 등)의 도입. 당장은 과한 느낌이 들 수 있지만, 서비스가 10개를 넘어가는 순간 "각 서버에 SSH 접속해서 로그 파일 뒤지기"는 현실적으로 불가능해집니다.

### 3. "156GB, 이걸로 충분한가?"

```
/dev/sda1       156G  147G  917M 100% /
/dev/sdb1       197G  179G  8.6G  96% /mnt/webhard
```

루트 파티션 156GB가 100%, 웹하드 파티션 197GB가 96%.

21GB 로그를 지워도 **135GB 이상이 뭔가로 차 있다**는 뜻입니다.

Docker 이미지 레이어, 오래된 프로젝트 빌드 아티팩트, 쓰이지 않는 snap 패키지들… 디스크를 잡아먹는 용의자는 많습니다.

```bash
# 안 쓰는 Docker 리소스 한 방 정리
$ docker system prune -af

# 대용량 파일 사냥
$ sudo find / -type f -size +100M -exec ls -lh {} + | sort -k 5 -rh | head -n 20
```

> 💡 **우리가 풀어야 할 숙제:** 정기적인 디스크 사용량 모니터링과 알림 설정. "100%가 되고 나서 알게 되는" 것이 아니라, **80%를 넘는 순간 슬랙에 알림이 오는** 구조가 필요합니다. health-check 봇이 서비스 상태를 감시하듯, 서버 리소스도 감시해야 합니다.

### 4. "서버가 재부팅되면?"

이번에 `docker-compose up -d`로 살린 서비스, 서버가 재부팅되면 자동으로 다시 올라올까요?

`docker-compose.prod.yaml`에 `restart: always` 옵션이 없다면, **안 올라옵니다.** PM2 없이 `nohup`으로 띄워둔 유령 프로세스들도 마찬가지고요.

> 💡 **우리가 확인해야 할 것:** 모든 서비스에 자동 재시작 정책이 적용되어 있는지. Docker면 `restart: always`, PM2면 `pm2 startup` + `pm2 save`가 설정되어 있는지.

---

## 📝 이번 장애에서 배운 것

정리하면, 이번 장애의 **Kill Chain**은 이렇습니다:

```
PM2 로그 무제한 적재 (수개월~수년)
    ↓
디스크 사용률 100% 도달
    ↓
Docker 컨테이너 I/O 실패 → 컨테이너 사망
    ↓
Nginx가 proxy_pass할 대상(8080) 소실
    ↓
502 Bad Gateway
    ↓
health-check 봇 경고
```

근본 원인은 단순합니다. **로그에 제한을 안 걸었다.** 하지만 이 단순한 원인이 30분간의 장애, 4명의 팀원 동원, 수십 분의 디버깅 시간을 만들어냈습니다.

그리고 더 무서운 건, 이번 장애가 **첫 번째가 아니었다**는 점입니다.

양현준이 슬랙에서 작년(2025년 2월)에도 같은 서버에서 디스크 100%로 인한 장애가 있었다는 스레드를 공유했습니다. **정확히 1년 전, 같은 원인, 같은 서버.**

> _"아 맞아요 저거 로그 주기적으로 지워줘야 할텐데"_

이미 알고 있었지만, 자동화되지 않았기에 반복된 장애.

**사람의 기억력에 의존하는 운영은 반드시 실패합니다.**

---

## 🛡️ Action Items

| 우선순위 | 과제                                               | 상태                 |
| -------- | -------------------------------------------------- | -------------------- |
| 🔴 긴급  | Docker daemon 로그 제한 설정 (`daemon.json`)       | 논의 완료, 적용 예정 |
| 🔴 긴급  | pm2-logrotate 설치 및 설정                         | ✅ 완료              |
| 🟡 중요  | 전체 서비스 인벤토리(Service Registry) 정리        | 미착수               |
| 🟡 중요  | 디스크 사용률 모니터링 + 슬랙 알림 구축            | 미착수               |
| 🟡 중요  | 유령 프로세스(8089, 8090, 8091) 정리 또는 PM2 이관 | 미착수               |
| 🟢 개선  | Node v10 root 프로세스 보안 점검 및 업그레이드     | 미착수               |
| 🟢 개선  | 중앙 로그 수집 시스템 도입 검토                    | 미착수               |
| 🟢 개선  | 전 서비스 자동 재시작(restart policy) 점검         | 미착수               |

---

## 에필로그

이번 월요일 오후의 30분은, SCG 인프라가 **"잘 돌아가고 있으니까 괜찮겠지"** 에서 **"왜 잘 돌아가고 있는지 알아야 한다"** 로 넘어가는 계기가 되었습니다.

25년 차 학생 개발 단체의 서버에는 여러 세대의 코드와 설정이 지층처럼 쌓여 있습니다. 누군가의 졸업 프로젝트, 누군가의 실험적 배포, 누군가의 "임시로 띄워둔" 서비스. 이것들이 관리의 손길 없이 오래 방치되면, 어느 월요일 오후에 디스크 한 칸의 여유도 없이 조용히 쓰러집니다.

**인프라는 화려하지 않습니다.** 잘 돌아갈 때는 아무도 고마워하지 않고, 뻗었을 때만 모두가 찾습니다. 하지만 그 "잘 돌아가는 상태"를 유지하는 건, 결국 로그 하나 제한 거는 것 같은 작고 지루한 일들의 축적입니다.

이번 장애 덕분에 우리는 그 작고 지루한 일들의 목록을 하나 더 만들었고, 하나씩 체크해 나갈 겁니다.

다음에 봇이 초록색 체크를 찍을 때, 그건 운이 아니라 시스템이 되어 있기를.

---

_Written by SCG 인프라팀_
_2026.02.23 장애 대응 포스트모템_
